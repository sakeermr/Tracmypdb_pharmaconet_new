{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PharmacoNet Batch Modeling - PRODUCTION READY VERSION\n",
    "# ============================================================================\n",
    "# Complete rewrite - all bugs fixed\n",
    "# 1. Enable T4 GPU in Runtime > Change runtime type\n",
    "# 2. Run this single cell\n",
    "# 3. Upload your CSV (columns: PDB_code, Ligand_ID)\n",
    "# 4. Get .pm pharmacophore model files\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHARMACONET BATCH PROCESSOR - PRODUCTION v2.0\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: GPU Check\n",
    "# ============================================================================\n",
    "print(\"STEP 1: Checking GPU...\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö† No GPU - will use CPU (slower)\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Install Dependencies\n",
    "# ============================================================================\n",
    "print(\"STEP 2: Installing dependencies...\")\n",
    "!apt-get update -qq > /dev/null 2>&1\n",
    "!apt-get install -y -qq libopenbabel-dev openbabel libglew-dev libpng-dev libxml2-dev libfreetype6-dev > /dev/null 2>&1\n",
    "\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q numpy==2.0.0 numba omegaconf molvoxel gdown tqdm rdkit openbabel-wheel biopython pandas psutil\n",
    "!pip install -q pymol-open-source\n",
    "!pip install -q git+https://github.com/SeonghwanSeo/PharmacoNet.git\n",
    "print(\"‚úì All dependencies installed\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Setup Working Directory\n",
    "# ============================================================================\n",
    "print(\"STEP 3: Setting up workspace...\")\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "WORK_DIR = Path(\"/content/pharmaconet_batch\")\n",
    "WORK_DIR.mkdir(exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "(WORK_DIR / \"input\").mkdir(exist_ok=True)\n",
    "(WORK_DIR / \"output\").mkdir(exist_ok=True)\n",
    "(WORK_DIR / \"utils\").mkdir(exist_ok=True)\n",
    "print(f\"‚úì Workspace: {WORK_DIR}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Create Python Modules\n",
    "# ============================================================================\n",
    "print(\"STEP 4: Generating Python modules...\")\n",
    "\n",
    "# Create utils/__init__.py\n",
    "(WORK_DIR / \"utils\" / \"__init__.py\").write_text(\"\")\n",
    "\n",
    "# Create utils/parse_rcsb_pdb.py\n",
    "parse_code = '''import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pymol\n",
    "from openbabel import pybel\n",
    "\n",
    "PathLike = str | Path\n",
    "\n",
    "@dataclass\n",
    "class LigandInform:\n",
    "    order: int\n",
    "    id: str\n",
    "    pdbchain: str\n",
    "    authchain: str\n",
    "    residx: int\n",
    "    center: tuple[float, float, float]\n",
    "    file_path: PathLike\n",
    "    name: str | None\n",
    "    synonyms: str | None\n",
    "\n",
    "def download_pdb(pdb_code: str, output_file: PathLike):\n",
    "    url = f\"https://files.rcsb.org/download/{pdb_code.lower()}.pdb\"\n",
    "    try:\n",
    "        with urlopen(url) as response:\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(response.read().decode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading PDB: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_pdb(pdb_code: str, protein_path: PathLike, save_dir: PathLike) -> list[LigandInform]:\n",
    "    try:\n",
    "        protein = next(pybel.readfile(\"pdb\", str(protein_path)))\n",
    "        if \"HET\" not in protein.data.keys():\n",
    "            return []\n",
    "        \n",
    "        het_lines = protein.data[\"HET\"].split(\"\\\\n\")\n",
    "        hetnam_lines = protein.data[\"HETNAM\"].split(\"\\\\n\")\n",
    "        hetsyn_lines = protein.data[\"HETSYN\"].split(\"\\\\n\") if \"HETSYN\" in protein.data.keys() else []\n",
    "        het_id_list = tuple(line.strip().split()[0] for line in het_lines)\n",
    "        \n",
    "        ligand_name_dict = {}\n",
    "        for line in hetnam_lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(het_id_list):\n",
    "                key, *strings = line.split()\n",
    "                ligand_name_dict[key] = \" \".join(strings)\n",
    "            else:\n",
    "                _, key, *strings = line.split()\n",
    "                if ligand_name_dict.get(key, \"\").endswith(\"-\"):\n",
    "                    ligand_name_dict[key] += \" \".join(strings)\n",
    "                else:\n",
    "                    ligand_name_dict[key] = ligand_name_dict.get(key, \"\") + \" \" + \" \".join(strings)\n",
    "        \n",
    "        pymol.finish_launching([\"pymol\", \"-cq\"])\n",
    "        pymol.cmd.load(str(protein_path))\n",
    "        ligand_inform_list = []\n",
    "        last_chain = protein.data[\"SEQRES\"].split(\"\\\\n\")[-1].split()[1]\n",
    "        \n",
    "        for idx, line in enumerate(het_lines):\n",
    "            vs = line.strip().split()\n",
    "            if len(vs) == 4:\n",
    "                ligid, authchain, residue_idx, _ = vs\n",
    "            else:\n",
    "                ligid, authchain, residue_idx = vs[0], vs[1][0], vs[1][1:]\n",
    "            \n",
    "            pdbchain = chr(ord(last_chain) + idx + 1)\n",
    "            identify_key = f\"{pdb_code}_{pdbchain}_{ligid}\"\n",
    "            ligand_path = os.path.join(save_dir, f\"{identify_key}.pdb\")\n",
    "            \n",
    "            if not os.path.exists(ligand_path):\n",
    "                pymol.cmd.select(identify_key, f\"resn {ligid} and resi {residue_idx} and chain {authchain}\")\n",
    "                pymol.cmd.save(ligand_path, identify_key)\n",
    "            \n",
    "            ligand = next(pybel.readfile(\"pdb\", ligand_path))\n",
    "            x, y, z = np.mean([atom.coords for atom in ligand.atoms], axis=0).tolist()\n",
    "            inform = LigandInform(idx + 1, ligid, pdbchain, authchain, int(residue_idx), (x, y, z), \n",
    "                                ligand_path, ligand_name_dict.get(ligid), None)\n",
    "            ligand_inform_list.append(inform)\n",
    "        \n",
    "        return ligand_inform_list\n",
    "    finally:\n",
    "        try:\n",
    "            pymol.cmd.reinitialize()\n",
    "            pymol.cmd.quit()\n",
    "        except:\n",
    "            pass\n",
    "'''\n",
    "(WORK_DIR / \"utils\" / \"parse_rcsb_pdb.py\").write_text(parse_code)\n",
    "\n",
    "# Create modeling.py - CLEAN VERSION\n",
    "modeling_code = '''#!/usr/bin/env python3\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pmnet\n",
    "from pmnet.module import PharmacoNet\n",
    "from pmnet.pharmacophore_model import PharmacophoreModel\n",
    "from utils.parse_rcsb_pdb import download_pdb, parse_pdb\n",
    "\n",
    "SUCCESS = 0\n",
    "EXIT = 1\n",
    "FAIL = 2\n",
    "NO_LIGAND = 3\n",
    "LIGAND_NOT_FOUND = 4\n",
    "\n",
    "def main(args):\n",
    "    PREFIX = args.prefix if args.prefix else args.pdb\n",
    "    SAVE_DIR = Path(args.out_dir) if args.out_dir else Path(\"./result\") / PREFIX\n",
    "    SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    module = PharmacoNet(\"cuda\" if args.cuda else \"cpu\", weight_path=args.weight_path)\n",
    "    logging.info(\"PharmacoNet loaded\")\n",
    "    \n",
    "    # Get protein file\n",
    "    if args.pdb:\n",
    "        protein_path = str(SAVE_DIR / f\"{PREFIX}.pdb\")\n",
    "        if not os.path.exists(protein_path):\n",
    "            logging.info(f\"Downloading {args.pdb}...\")\n",
    "            download_pdb(args.pdb, protein_path)\n",
    "        else:\n",
    "            logging.info(f\"Using cached {protein_path}\")\n",
    "    elif args.protein:\n",
    "        protein_path = args.protein\n",
    "        if not os.path.exists(protein_path):\n",
    "            raise Exception(f\"Protein file not found: {protein_path}\")\n",
    "    else:\n",
    "        raise Exception(\"Missing --pdb or --protein\")\n",
    "    \n",
    "    # Parse ligands\n",
    "    inform_list = parse_pdb(PREFIX, protein_path, SAVE_DIR)\n",
    "    \n",
    "    if len(inform_list) == 0:\n",
    "        logging.warning(\"No ligands detected in PDB\")\n",
    "        return NO_LIGAND\n",
    "    \n",
    "    # Filter by ligand_id if specified\n",
    "    if args.ligand_id:\n",
    "        original_count = len(inform_list)\n",
    "        inform_list = [inf for inf in inform_list if inf.id.upper() == args.ligand_id.upper()]\n",
    "        if len(inform_list) == 0:\n",
    "            logging.warning(f\"Ligand {args.ligand_id} not found (had {original_count} ligands)\")\n",
    "            return LIGAND_NOT_FOUND\n",
    "        logging.info(f\"Filtered to ligand {args.ligand_id}\")\n",
    "    \n",
    "    # Process each ligand\n",
    "    for inform in inform_list:\n",
    "        model_path = SAVE_DIR / f\"{PREFIX}_{inform.pdbchain}_{inform.id}_model.{args.suffix}\"\n",
    "        \n",
    "        if (not args.force) and os.path.exists(model_path):\n",
    "            logging.info(f\"Skipping {model_path} (exists)\")\n",
    "            continue\n",
    "        \n",
    "        logging.info(f\"Processing ligand {inform.id} (chain {inform.pdbchain})...\")\n",
    "        pharmacophore_model = module.run(protein_path, ref_ligand_path=inform.file_path, center=inform.center)\n",
    "        pharmacophore_model.save(str(model_path))\n",
    "        logging.info(f\"Saved {model_path}\")\n",
    "    \n",
    "    return SUCCESS\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--pdb\", type=str)\n",
    "    parser.add_argument(\"--ligand_id\", type=str)\n",
    "    parser.add_argument(\"--protein\", type=str)\n",
    "    parser.add_argument(\"--out_dir\", type=str)\n",
    "    parser.add_argument(\"--prefix\", type=str)\n",
    "    parser.add_argument(\"--suffix\", choices=(\"pm\", \"json\"), default=\"pm\")\n",
    "    parser.add_argument(\"--weight_path\", type=str)\n",
    "    parser.add_argument(\"--cuda\", action=\"store_true\")\n",
    "    parser.add_argument(\"--force\", action=\"store_true\")\n",
    "    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "    \n",
    "    try:\n",
    "        sys.exit(main(args))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FATAL ERROR: {e}\")\n",
    "        sys.exit(FAIL)\n",
    "'''\n",
    "(WORK_DIR / \"modeling.py\").write_text(modeling_code)\n",
    "\n",
    "# Create batch_modeling.py - CLEAN VERSION\n",
    "batch_code = '''#!/usr/bin/env python3\n",
    "import argparse\n",
    "import csv\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Aggressive cleanup\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    try:\n",
    "        subprocess.run(['pkill', '-f', 'pymol'], stderr=subprocess.DEVNULL, timeout=2)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def get_memory_stats():\n",
    "    \"\"\"Get memory info\"\"\"\n",
    "    stats = []\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            alloc = torch.cuda.memory_allocated() / 1024**3\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            stats.append(f\"GPU: {alloc:.1f}/{total:.1f}GB\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        import psutil\n",
    "        mem = psutil.virtual_memory()\n",
    "        stats.append(f\"RAM: {mem.percent:.0f}%\")\n",
    "    except:\n",
    "        pass\n",
    "    return \" | \".join(stats)\n",
    "\n",
    "def parse_csv(csv_path):\n",
    "    \"\"\"Parse CSV and return list of entries\"\"\"\n",
    "    entries = []\n",
    "    with open(csv_path, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            row_norm = {k.lower().strip(): v.strip() if v else None for k, v in row.items()}\n",
    "            pdb = row_norm.get('pdb_code') or row_norm.get('pdb')\n",
    "            ligand = row_norm.get('ligand_id') or row_norm.get('ligand')\n",
    "            if pdb:\n",
    "                entries.append({'pdb': pdb.upper(), 'ligand': ligand.upper() if ligand else None})\n",
    "    return entries\n",
    "\n",
    "def check_exists(pdb, ligand, output_dir, suffix):\n",
    "    \"\"\"Check if output already exists\"\"\"\n",
    "    pdb_dir = output_dir / pdb\n",
    "    if not pdb_dir.exists():\n",
    "        return False\n",
    "    pattern = f\"{pdb}_*_{ligand}_model.{suffix}\" if ligand else f\"{pdb}_*_model.{suffix}\"\n",
    "    return len(list(pdb_dir.glob(pattern))) > 0\n",
    "\n",
    "def save_summary(successful, timeout, no_ligand, not_found, failed, skipped, output_dir, start, end):\n",
    "    \"\"\"Save summary.txt\"\"\"\n",
    "    with open(output_dir / \"summary.txt\", 'w') as f:\n",
    "        f.write(\"=\"*70 + \"\\\\n\")\n",
    "        f.write(\"PHARMACONET BATCH RESULTS\\\\n\")\n",
    "        f.write(\"=\"*70 + \"\\\\n\\\\n\")\n",
    "        f.write(f\"Started:  {start.strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "        f.write(f\"Finished: {end.strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "        f.write(f\"Duration: {end - start}\\\\n\\\\n\")\n",
    "        \n",
    "        total = len(successful) + len(timeout) + len(no_ligand) + len(not_found) + len(failed) + len(skipped)\n",
    "        f.write(f\"Total:            {total}\\\\n\")\n",
    "        f.write(f\"Successful:       {len(successful)}\\\\n\")\n",
    "        f.write(f\"Timeout (>30min): {len(timeout)}\\\\n\")\n",
    "        f.write(f\"No Ligands:       {len(no_ligand)}\\\\n\")\n",
    "        f.write(f\"Ligand Not Found: {len(not_found)}\\\\n\")\n",
    "        f.write(f\"Failed:           {len(failed)}\\\\n\")\n",
    "        f.write(f\"Skipped (cached): {len(skipped)}\\\\n\\\\n\")\n",
    "        \n",
    "        if successful:\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            f.write(f\"SUCCESSFUL ({len(successful)})\\\\n\")\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            for e in successful:\n",
    "                f.write(f\"  ‚úì {e['pdb']}\" + (f\" - {e['ligand']}\" if e['ligand'] else \"\") + \"\\\\n\")\n",
    "            f.write(\"\\\\n\")\n",
    "        \n",
    "        if timeout:\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            f.write(f\"TIMEOUT - EXCEEDED 30 MINUTES ({len(timeout)})\\\\n\")\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            for e in timeout:\n",
    "                f.write(f\"  ‚è± {e['pdb']}\" + (f\" - {e['ligand']}\" if e['ligand'] else \"\") + \"\\\\n\")\n",
    "            f.write(\"\\\\n\")\n",
    "        \n",
    "        if no_ligand:\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            f.write(f\"NO LIGANDS IN PDB ({len(no_ligand)})\\\\n\")\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            for e in no_ligand:\n",
    "                f.write(f\"  ‚äò {e['pdb']}\\\\n\")\n",
    "            f.write(\"\\\\n\")\n",
    "        \n",
    "        if not_found:\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            f.write(f\"LIGAND NOT FOUND ({len(not_found)})\\\\n\")\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            for e in not_found:\n",
    "                f.write(f\"  ‚äò {e['pdb']} - requested: {e['ligand']}\\\\n\")\n",
    "            f.write(\"\\\\n\")\n",
    "        \n",
    "        if failed:\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            f.write(f\"FAILED ({len(failed)})\\\\n\")\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            for e in failed:\n",
    "                f.write(f\"  ‚úó {e['pdb']}\" + (f\" - {e['ligand']}\" if e['ligand'] else \"\"))\n",
    "                if 'error' in e:\n",
    "                    f.write(f\" ({e['error']})\")\n",
    "                f.write(\"\\\\n\")\n",
    "            f.write(\"\\\\n\")\n",
    "        \n",
    "        if skipped:\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            f.write(f\"SKIPPED - ALREADY EXISTS ({len(skipped)})\\\\n\")\n",
    "            f.write(\"=\"*70 + \"\\\\n\")\n",
    "            for e in skipped:\n",
    "                f.write(f\"  ‚äô {e['pdb']}\" + (f\" - {e['ligand']}\" if e['ligand'] else \"\") + \"\\\\n\")\n",
    "\n",
    "def main(args):\n",
    "    start_time = datetime.now()\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Parse CSV\n",
    "    entries = parse_csv(args.input_csv)\n",
    "    logging.info(f\"Loaded {len(entries)} entries from CSV\")\n",
    "    \n",
    "    # Check for existing\n",
    "    skipped = []\n",
    "    if args.skip_existing and not args.force:\n",
    "        to_process = []\n",
    "        for entry in entries:\n",
    "            if check_exists(entry['pdb'], entry['ligand'], output_dir, args.suffix):\n",
    "                skipped.append(entry)\n",
    "            else:\n",
    "                to_process.append(entry)\n",
    "        if skipped:\n",
    "            logging.info(f\"Skipping {len(skipped)} already completed\")\n",
    "        entries = to_process\n",
    "    \n",
    "    if not entries:\n",
    "        logging.info(\"All entries complete!\")\n",
    "        save_summary([], [], [], [], [], skipped, output_dir, start_time, datetime.now())\n",
    "        return\n",
    "    \n",
    "    logging.info(f\"Processing {len(entries)} entries (30min timeout each)\")\n",
    "    logging.info(\"\")\n",
    "    \n",
    "    successful = []\n",
    "    timeout_list = []\n",
    "    no_ligand = []\n",
    "    not_found = []\n",
    "    failed = []\n",
    "    \n",
    "    for i, entry in enumerate(entries, 1):\n",
    "        pdb = entry['pdb']\n",
    "        ligand = entry['ligand']\n",
    "        desc = f\"{pdb}\" + (f\" (ligand: {ligand})\" if ligand else \"\")\n",
    "        \n",
    "        logging.info(\"=\"*70)\n",
    "        logging.info(f\"[{i}/{len(entries)}] {desc}\")\n",
    "        logging.info(\"=\"*70)\n",
    "        \n",
    "        if i % 5 == 0 or i == 1:\n",
    "            logging.info(f\"Memory: {get_memory_stats()}\")\n",
    "        \n",
    "        # Build command\n",
    "        cmd = [sys.executable, \"modeling.py\", \"--pdb\", pdb, \"--out_dir\", str(output_dir / pdb), \n",
    "               \"--suffix\", args.suffix]\n",
    "        if ligand:\n",
    "            cmd.extend([\"--ligand_id\", ligand])\n",
    "        if args.cuda:\n",
    "            cmd.append(\"--cuda\")\n",
    "        if args.force:\n",
    "            cmd.append(\"--force\")\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(cmd, check=False, capture_output=True, text=True, \n",
    "                                  timeout=1800, cwd=str(Path.cwd()))\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                successful.append(entry)\n",
    "                logging.info(\"‚úì SUCCESS\")\n",
    "            elif result.returncode == 3:\n",
    "                no_ligand.append(entry)\n",
    "                logging.warning(\"‚äò No ligands in PDB\")\n",
    "            elif result.returncode == 4:\n",
    "                not_found.append(entry)\n",
    "                logging.warning(f\"‚äò Ligand {ligand} not found\")\n",
    "            else:\n",
    "                entry_err = entry.copy()\n",
    "                entry_err['error'] = f\"Exit code {result.returncode}\"\n",
    "                failed.append(entry_err)\n",
    "                logging.error(f\"‚úó FAILED (exit {result.returncode})\")\n",
    "                if result.stderr:\n",
    "                    logging.error(result.stderr[-500:])\n",
    "        \n",
    "        except subprocess.TimeoutExpired:\n",
    "            timeout_list.append(entry)\n",
    "            logging.warning(\"‚è± TIMEOUT (>30min) - Continuing to next entry...\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            entry_err = entry.copy()\n",
    "            entry_err['error'] = str(e)\n",
    "            failed.append(entry_err)\n",
    "            logging.error(f\"‚úó ERROR: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            cleanup()\n",
    "            if i % 5 == 0:\n",
    "                logging.info(\"üßπ Cleanup done\")\n",
    "        \n",
    "        logging.info(\"\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    save_summary(successful, timeout_list, no_ligand, not_found, failed, skipped, output_dir, start_time, end_time)\n",
    "    \n",
    "    logging.info(\"=\"*70)\n",
    "    logging.info(\"BATCH COMPLETE\")\n",
    "    logging.info(\"=\"*70)\n",
    "    logging.info(f\"Total: {len(entries) + len(skipped)}\")\n",
    "    logging.info(f\"Successful: {len(successful)}\")\n",
    "    logging.info(f\"Timeout: {len(timeout_list)}\")\n",
    "    logging.info(f\"Failed: {len(failed) + len(no_ligand) + len(not_found)}\")\n",
    "    logging.info(f\"Skipped: {len(skipped)}\")\n",
    "    logging.info(f\"Summary: {output_dir}/summary.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_csv\", type=str, default=\"input/pdb_list.csv\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"output\")\n",
    "    parser.add_argument(\"--suffix\", choices=(\"pm\", \"json\"), default=\"pm\")\n",
    "    parser.add_argument(\"--cuda\", action=\"store_true\")\n",
    "    parser.add_argument(\"--force\", action=\"store_true\")\n",
    "    parser.add_argument(\"--skip_existing\", action=\"store_true\", default=True)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "    main(args)\n",
    "'''\n",
    "(WORK_DIR / \"batch_modeling.py\").write_text(batch_code)\n",
    "!chmod +x {WORK_DIR}/batch_modeling.py\n",
    "\n",
    "print(\"‚úì Created utils/parse_rcsb_pdb.py\")\n",
    "print(\"‚úì Created modeling.py\")\n",
    "print(\"‚úì Created batch_modeling.py\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Upload CSV\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 5: Upload your CSV file\")\n",
    "print(\"=\"*70)\n",
    "print(\"Required columns: PDB_code, Ligand_ID\")\n",
    "print(\"Example:\")\n",
    "print(\"  PDB_code,Ligand_ID\")\n",
    "print(\"  1A2B,HEM\")\n",
    "print(\"  3C4D,ATP\")\n",
    "print()\n",
    "\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    csv_file = list(uploaded.keys())[0]\n",
    "    shutil.move(csv_file, WORK_DIR / \"input\" / \"pdb_list.csv\")\n",
    "    print(f\"‚úì CSV uploaded: {WORK_DIR}/input/pdb_list.csv\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"‚ùå No file uploaded - cannot continue\")\n",
    "    raise SystemExit(\"Upload cancelled\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Run Batch Processing\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 6: Running batch processing\")\n",
    "print(\"=\"*70)\n",
    "print(\"Configuration:\")\n",
    "print(\"  ‚Ä¢ Timeout: 30 minutes per entry (non-fatal)\")\n",
    "print(\"  ‚Ä¢ Output: .pm pharmacophore models only\")\n",
    "print(\"  ‚Ä¢ PyMOL visualization: DISABLED (for speed)\")\n",
    "print(\"  ‚Ä¢ Memory cleanup: After every entry\")\n",
    "print(\"  ‚Ä¢ Auto-skip: Previously completed entries\")\n",
    "print()\n",
    "print(\"Starting batch job...\")\n",
    "print()\n",
    "\n",
    "!cd /content/pharmaconet_batch && python batch_modeling.py \\\n",
    "    --input_csv input/pdb_list.csv \\\n",
    "    --output_dir output \\\n",
    "    --cuda \\\n",
    "    --skip_existing\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Download Results\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 7: Download results\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_path = WORK_DIR / \"output\" / \"summary.txt\"\n",
    "if summary_path.exists():\n",
    "    print(\"üìÑ Summary Report:\")\n",
    "    print(\"-\" * 70)\n",
    "    with open(summary_path) as f:\n",
    "        print(f.read())\n",
    "    print(\"-\" * 70)\n",
    "else:\n",
    "    print(\"‚ö† No summary.txt found\")\n",
    "\n",
    "print()\n",
    "print(\"Creating ZIP archive...\")\n",
    "output_zip = \"/content/pharmacophore_results.zip\"\n",
    "shutil.make_archive(output_zip.replace('.zip', ''), 'zip', WORK_DIR / \"output\")\n",
    "print(f\"‚úì Created {output_zip}\")\n",
    "\n",
    "files.download(output_zip)\n",
    "print(\"‚úì Download started!\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Results location: {WORK_DIR}/output/\")\n",
    "print(f\"Summary report: {WORK_DIR}/output/summary.txt\")\n",
    "print()\n",
    "print(\"üí° TIP: Re-run this cell to resume - already completed entries are skipped\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
